{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035c4e27-fc62-430d-af99-7f69ae1271dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Load and preparing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0dadc-5d06-48ef-8f56-0e8957f0a59d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "no_subjects = 35\n",
    "# Load the variables from the pickle file\n",
    "with open(\"Cond_35_12b_adap_mine2.pkl\", \"rb\") as file:\n",
    "    powers_c, powers_o, center_gravity_c, center_gravity_o, target = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "391488da-9530-4522-b9ce-41a5ede0c340",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "II =[3,4,5]\n",
    "medians_c = np.median(powers_c,axis=(2))\n",
    "median_c_g_c = np.median(center_gravity_c,axis=(2))\n",
    "median_c_g_o = np.median(center_gravity_o,axis=(2))\n",
    "medians_o = np.median(powers_o,axis=(2))#(1,2))\n",
    "diff = np.concatenate((medians_c,medians_o,medians_c-medians_o), axis=2) \n",
    "DDFS = [26,25,36,25,33,26,32,28,40,28,25,22,26,25,23,27,33,44,60,23,44,17,39,49,41]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c32703-ddd5-48ee-9b96-2d3a86d919be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the variables from the pickle file\n",
    "with open(\"Cond_35_12b_adap_mine2.pkl\", \"rb\") as file:\n",
    "    powers_c, powers_o, center_gravity_c, center_gravity_o, targets= pickle.load(file)\n",
    "\n",
    "II =[3,4,5]\n",
    "medians_c = np.median(powers_c,axis=(2))#(1,2))#median\n",
    "median_c_g_c = np.median(center_gravity_c,axis=(2))\n",
    "median_c_g_o = np.median(center_gravity_o,axis=(2))\n",
    "medians_o = np.median(powers_o,axis=(2))#(1,2))\n",
    "diff = np.concatenate((medians_c,medians_o,medians_c-medians_o), axis=2) \n",
    "DDFS = [26,25,36,25,33,26,32,28,40,28,25,22,26,25,23,27,33,44,60,23,44,17,39,49,41]\n",
    "\n",
    "\n",
    "#no_sbjects = 31\n",
    "import matplotlib.pyplot as plt\n",
    "no_blocks = 12\n",
    "no_channels = 3\n",
    "rej_no = 0\n",
    "II = [4,13,36]\n",
    "prc = 1\n",
    "\n",
    "\n",
    "sbj_id = np.ones((no_subjects-rej_no)*no_blocks*prc)\n",
    "sbj_blocks =  np.ones((no_subjects-rej_no)*no_blocks*prc)\n",
    "block_time =  np.ones((no_subjects-rej_no)*no_blocks*prc)\n",
    "\n",
    "for si in range(rej_no,no_subjects):\n",
    "    sbj_id[(si-rej_no)*no_blocks*prc:(si-rej_no)*prc*no_blocks+no_blocks*prc]=si#\n",
    "    sbj_blocks[(si-rej_no)*no_blocks*prc:(si-rej_no)*no_blocks*prc+no_blocks*prc]= np.repeat(np.arange(1, no_blocks + 1), prc)\n",
    "    block_time[(si-rej_no)*no_blocks*prc:(si-rej_no)*no_blocks*prc+no_blocks*prc]= np.repeat(ages[si],prc)#[8.05,12.05,16.05,20.05, 00.05,4.05,20.05,00.05,4.05,8.05,12.05,16.05]\n",
    "    \n",
    "X = diff[rej_no*prc:,:,:].reshape(((no_subjects-rej_no)*prc*no_blocks,3*4*no_channels))# medians[:,1,II]- medians[:,0,II].reshape((no_subjects,2*no_channels))\n",
    "#ddfs =  cat_DDFS[rej_no:,:]\n",
    "#ddfs = ddfs.reshape((no_subjects-rej_no)*no_blocks)\n",
    "\n",
    "#Subject 9 and 13 new values\n",
    "target[8,:] = np.array([154.5337899,\t157.8603336,\t148.5623349,\t153.6006809,\t283.3165515,\t326.2169195,\t123.5740912,\t140.1647044,\t895.3080837,188.6211716,167.2686405,\t199.4810817\t])/10\n",
    "target[12,:] = np.array([140.9569312,\t144.5036967,\t153.9935119,\t146.8333177,\t142.5171401,\t153.5668623,\t146.1177533,\t135.1877086,\t149.701896,175.9445015,176.0910196,\t172.6305761])/10\t\n",
    "#NAN values\n",
    "target[9,9] = 280.907242/10;\n",
    "target[9,5] = 607.913013/10\t\n",
    "target[6,0]= 271.754541/10\t\n",
    "target[6,8]= 254.777298/10\n",
    "\n",
    "target[11,1] = 179.802235/10\n",
    "target[11,3] = 198.65912/10;\n",
    "target[11,7] = 226.975496/10\n",
    "target[4,3] =  287.913876/10;\n",
    "target[4,7] =  333.41579/10;\n",
    "target[4,8] =  378.425934/10\n",
    "target[1,10] = np.nan\n",
    "target[2,10] = np.nan\n",
    "target[8,10] = np.nan\n",
    "target[30,10] = np.nan\n",
    "\n",
    "#SE2\n",
    "#target[2,0] = np.nan;target[2,1]=np.nan;target[2,2]=np.nan;target[2,3]=np.nan;target[2,4]=np.nan;target[2,5]=np.nan;target[2,6]=np.nan; target[2,7]=np.nan;target[2,8]=np.nan\n",
    "\n",
    "target[3,4] = np.nan; target[3,5] = np.nan; target[3,7]= np.nan; target[3,8] = np.nan\n",
    "target[5,5] = np.nan; target[5,8]= np.nan\n",
    "target[11,5] = np.nan\n",
    "target[17,8] = np.nan\n",
    "\n",
    "I9  = [0,4,5,7,13,14,15,16,18,20,22,27,28,29,30,31,33,34]\n",
    "I10 = [10,7,2,3,6,9,17,19,21,23,24,26]\n",
    "I11 = [8,10,11,12,25,32]\n",
    "\n",
    "target[3,:] = np.nan\n",
    "target[22,:] = np.nan\n",
    "for si in range(rej_no,no_subjects):\n",
    "    if  ages[si]<30 or ages[si]>40:\n",
    "        target[si,:] = np.nan\n",
    "\n",
    "target = np.repeat(target, prc, axis=0)\n",
    "y = target[rej_no*prc:,:]\n",
    "y = y.reshape((no_subjects-rej_no) * prc*no_blocks)\n",
    "# Check which rows in the target data are not NaN\n",
    "non_nan_indices =  ~np.isnan(y)#range(0,len(y))#\n",
    "\n",
    "\n",
    "# Filter both X and y using those indices\n",
    "input_data = X[non_nan_indices,:]\n",
    "targets_tmp = y[non_nan_indices]\n",
    "targets = y[non_nan_indices]\n",
    "block_time = block_time[non_nan_indices]\n",
    "print(target.shape)\n",
    "print(targets.shape)\n",
    "#ddfs = ddfs[non_nan_indices]\n",
    "sbj_ids = sbj_id[non_nan_indices]\n",
    "sbj_blocks = sbj_blocks[non_nan_indices]\n",
    "\n",
    "center_g_c = median_c_g_c [rej_no*prc:,:,:].reshape((no_subjects-rej_no) * prc*no_blocks,no_channels)[non_nan_indices,:]\n",
    "center_g_o = median_c_g_o[rej_no*prc:,:,:].reshape((no_subjects-rej_no) *prc* no_blocks,no_channels)[non_nan_indices,:]\n",
    "\n",
    "low_threshold = np.percentile((targets[~np.isnan(targets)]), 33)  #np.percentile(np.unique(targets[~np.isnan(targets)]), 33)  # 33rd percentile\n",
    "mid_threshold = np.percentile((targets[~np.isnan(targets)]), 66)  ##np.percentile(np.unique(targets[~np.isnan(targets)]), 66)  #66rd percentile\n",
    "\n",
    "# Function to categorize the values\n",
    "def categorize(value):\n",
    "    \n",
    "    if(np.isnan(value)):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if value <= low_threshold:\n",
    "            return 0 # Low\n",
    "        elif value <=  mid_threshold:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return 1\n",
    "print(low_threshold)\n",
    "print(mid_threshold)\n",
    "# Apply the function to each value in the targets array\n",
    "targets = np.array([categorize(val) for val in targets])\n",
    "non_nan_indices = ~np.isnan(targets)## range(0,len(y))#\n",
    "\n",
    "## Filter both X and y using those indices\n",
    "input_data = input_data[non_nan_indices,:]\n",
    "targets = targets[non_nan_indices]\n",
    "targets_tmp = targets_tmp[non_nan_indices]\n",
    "sbj_ids = sbj_ids[non_nan_indices]\n",
    "sbj_blocks = sbj_blocks[non_nan_indices]\n",
    "block_time = block_time[non_nan_indices]\n",
    "\n",
    "center_g_o = center_g_o[non_nan_indices,:]\n",
    "center_g_c = center_g_c[non_nan_indices,:]\n",
    "\n",
    "\n",
    "print('-------------')\n",
    "print(len(np.where(targets==0)[0]))\n",
    "print(len(np.where(targets==1)[0]))\n",
    "print(len(np.unique(sbj_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6584bf7-f5a2-409a-9cb6-673dc224d06f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Mutual Info between features and target\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "II = np.array([-1,0,3,6,9,12,15,18,21,27,30,33,36,39,42])+1#\n",
    "# Assuming X is your feature matrix and y is your target vector\n",
    "power_mind = input_data#np.concatenate((input_data,mind_wondering.reshape(len(mind_wondering),1)),axis=1)#mind_wondering\n",
    "\n",
    "power_mind_g = np.concatenate((power_mind,center_g_c ), axis=1)\n",
    "power_mind_g_oc = np.concatenate((power_mind_g,center_g_o ), axis=1)\n",
    "power_mind_g_oc = np.concatenate((power_mind_g_oc,center_g_c-center_g_o), axis=1)\n",
    "bt =  np.expand_dims(block_time, axis=1)\n",
    "power_mind_g_oc = np.concatenate((power_mind_g_oc ,bt), axis=1)\n",
    "\n",
    "cls=['Alpha,Cz,c','Alpha,Pz,c','Alpha,Fz,c','Beta,Cz,c','Beta,Pz,c','Beta,Fz,c','Theta,Cz,c','Theta,Pz,c','Theta,Fz,c','Delta,Cz,c','Delta,Pz,c','Delta,Fz,c','Alpha,Cz,o','Alpha,Pz,o','Alpha,Fz,o','Beta,Cz,o','Beta,Pz,o','Beta,Fz,o','Theta,Cz,o','Theta,Pz,o','Theta,Fz,o','Delta,Cz,o','Delta,Pz,o','Delta,Fz,o','dif_Alpha,Cz,c-o','dif_Alpha,Pz,c-o','dif_Alpha,Fz,c-o','dif_Beta,Cz,c-o','dif_Beta,Pz,c-o','dif_Beta,Fz,c-o','dif_Theta,Cz,c-o','dif-Theta,Pz,c-o','dif-Theta,Fz,c-o','dif-Delta,Cz,c-o','dif-Delta,Pz,c-o','dif-Delta,Fz,c-o','Freq_Grav_Cz_c','Freq_Grav_Pz_c','Freq_Grav_Fz_c','Freq_Grav_Cz_o','Freq_Grav_Pz_o','Freq_Grav_Fz_o','dif-Freq_Grav_Cz_c-o','Age','dif-Freq_Grav_Fz_c-o','Time']\n",
    "X = pd.DataFrame(power_mind_g_oc[:,:44], columns=cls[:44])#[cls[i] for i in II])\n",
    "X.iloc[:,43] = block_time\n",
    "mi = mutual_info_classif(X, targets, random_state=0)\n",
    "\n",
    "# Convert to a DataFrame for better interpretation\n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi})\n",
    "mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "print(mi_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2026b-5f35-4509-8f86-b3ef1bd4af4c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVM classifere training and evaluations\n",
    "from xgboost import XGBRegressor  # Assuming driving performance is continuous\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cm = np.zeros((2,2))\n",
    "\n",
    "\n",
    "m = power_mind_g_oc[:,:].shape[0]#input_data.shape[0]\n",
    "n = 1#input_data.shape[1]\n",
    "p = power_mind_g_oc.shape[1]\n",
    "\n",
    "acc = []\n",
    "ytests = []\n",
    "preds = []\n",
    "\n",
    "# Initialize the XGBoost regressor (for regression tasks)\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=np.unique(sbj_ids[:]).shape[0]) # We have sleep parameters of 16 subjects\n",
    "\n",
    "# List to store the root mean squared error for each fold\n",
    "II =  [43,10,3,4,5,9,7,1,33]\n",
    "data = power_mind_g_oc[:,II]\n",
    "re_targets = targets\n",
    "re_sbj_ids = sbj_ids\n",
    "n_match = 0; n_total = 0\n",
    "i =1\n",
    "preds=[]\n",
    "all_test_blocks = []\n",
    "y_tests=[]\n",
    "feature_names =['Alpha,Cz,c','Alpha,Pz,c','Alpha,Fz,c','Beta,Cz,c','Beta,Pz,c','Beta,Fz,c','Theta,Cz,c','Theta,Pz,c','Theta,Fz,c','Delta,Cz,c','Delta,Pz,c','Delta,Fz,c','Alpha,Cz,o','Alpha,Pz,o','Alpha,Fz,o','Beta,Cz,o','Beta,Pz,o','Beta,Fz,o','Theta,Cz,o','Theta,Pz,o','Theta,Fz,o','Delta,Cz,o','Delta,Pz,o','Delta,Fz,o','dif_Alpha,Cz,c-o','dif_Alpha,Pz,c-o','dif_Alpha,Fz,c-o','dif_Beta,Cz,c-o','dif_Beta,Pz,c-o','dif_Beta,Fz,c-o','dif_Theta,Cz,c-o','dif-Theta,Pz,c-o','dif-Theta,Fz,c-o','dif-Delta,Cz,c-o','dif-Delta,Pz,c-o','dif-Delta,Fz,c-o','Freq_Grav_Cz_c','Freq_Grav_Pz_c','Freq_Grav_Fz_c','Freq_Grav_Cz_o','Freq_Grav_Pz_o','Freq_Grav_Fz_o','dif-Freq_Grav_Cz_c-o','Age','dif-Freq_Grav_Fz_c-o','Time']\n",
    "feature_names = [feature_names[i] for i in II]\n",
    "# Perform cross-validation manually\n",
    "for train_index, test_index in group_kfold.split(data, re_targets, re_sbj_ids):\n",
    "    print('test_sbj')\n",
    "    print(sbj_ids[test_index])\n",
    "    X_train, X_test =  data[train_index,:], data[test_index,:]\n",
    "    y_train, y_test =  re_targets[train_index],  re_targets[test_index]\n",
    "    test_blocks = sbj_blocks[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    train_sbjs = re_sbj_ids[train_index]\n",
    "    test_blocks = sbj_blocks[test_index]\n",
    "    \n",
    "   # np.random.seed(42)\n",
    "    \n",
    "    rows_with_1_in_first = np.where(y_train == 0)[0]\n",
    "    rows_with_1_in_mid = np.where(y_train == 1)[0]\n",
    "    # Find rows where the second column is 1\n",
    "    rows_with_1_in_second = np.where(y_train == 1)[0]\n",
    "    rows_with_similarity = np.where(X_train[:,-1] ==  X_test[0,-1])[0]\n",
    "    \n",
    "    # Randomly select 5 indices from each group\n",
    "    random_rows_with_1_in_first_5 = np.random.choice(rows_with_1_in_first,2, replace=False)\n",
    "    random_rows_with_1_in_second_5 = np.random.choice(rows_with_1_in_second,2, replace=False)\n",
    "    \n",
    "    #random_rows_with_similarity = np.random.choice(rows_with_similarity,,39\n",
    "    X_val = np.concatenate(( X_train[ random_rows_with_1_in_first_5,:],X_train[random_rows_with_1_in_second_5,:]),axis=0)\n",
    "    y_val = np.concatenate(( y_train[ random_rows_with_1_in_first_5],y_train[random_rows_with_1_in_second_5]),axis=0)\n",
    "    X_train = np.delete(X_train, [  random_rows_with_1_in_first_5,random_rows_with_1_in_second_5 ], axis=0)\n",
    "    y_train = np.delete(y_train, [  random_rows_with_1_in_first_5,random_rows_with_1_in_second_5], axis=0)\n",
    "\n",
    "    if not (0 in y_train and 1 in y_train):\n",
    "        print(\"Skipping this fold because one of the classes is missing in y_train\")\n",
    "        continue  # Move to \n",
    "\n",
    "    svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    preds.append(y_pred)\n",
    "    y_tests.append(y_test)\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    n_match += np.sum(y_test==y_pred.T)\n",
    "    n_total += (y_pred).shape[0]\n",
    "    print('-------------------------')\n",
    "    print((y_test==y_pred.T).shape)\n",
    "    print(y_pred.shape)\n",
    "    print(y_test.shape)\n",
    "    print('--------------------------')\n",
    "    #print(confusion_matrix(y_test, y_pred))\n",
    "    #cm += confusion_matrix(y_test, y_pred)\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate the Root Mean Squared Error for this fold\n",
    "    acc.append(accuracy)\n",
    "    all_test_blocks.append(test_blocks)\n",
    "    \n",
    "    print(f\"Subject {i}, Fold Accuracy: {accuracy}\")\n",
    "    class_mapping = {0: \"High Per\", 1: \"Low per\"}\n",
    "  \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Flatten the list of arrays into one 1D array\n",
    "y_tests_flat = np.concatenate(y_tests)\n",
    "preds_flat = np.concatenate(preds)\n",
    "# Now compute the classification report\n",
    "print(\"=== Overall Classification Report ===\")\n",
    "print(classification_report(y_tests_flat, preds_flat))\n",
    "print(np.sum(preds_flat==y_tests_flat)/len(y_tests_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef24fea-7db5-4f82-82d2-f321214b9968",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cat-boost Classifier training and evaluations\n",
    "\n",
    "from xgboost import XGBRegressor  # Assuming driving performance is continuous\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "cm = np.zeros((2,2))\n",
    "\n",
    "\n",
    "m = power_mind_g_oc[:,:].shape[0]#input_data.shape[0]\n",
    "n = 1#input_data.shape[1]\n",
    "p = power_mind_g_oc.shape[1]\n",
    "\n",
    "acc = []\n",
    "ytests = []\n",
    "preds = []\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,       # Number of boosting iterations\n",
    "    learning_rate = 0.15,     # Learning rate\n",
    "    l2_leaf_reg = 15,\n",
    "    depth = 5,               # Depth of the tree\n",
    "    loss_function='MultiClass',  # Multi-class classification\n",
    "    verbose=100,     # Output every 100 iterations\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=np.unique(sbj_ids[:]).shape[0]) # We have sleep parameters of 16 subjects\n",
    "\n",
    "# List to store the root mean squared error for each fold\n",
    "II = [43,10,4,9,7,36,1,37,33,31]# Selected features index\n",
    "re_targets = targets \n",
    "re_sbj_ids = sbj_ids\n",
    "n_match = 0; n_total = 0\n",
    "i = 1\n",
    "preds = []\n",
    "all_test_blocks = []\n",
    "y_tests = []\n",
    "#All features labels\n",
    "feature_names =['Alpha,Cz,c','Alpha,Pz,c','Alpha,Fz,c','Beta,Cz,c','Beta,Pz,c','Beta,Fz,c','Theta,Cz,c','Theta,Pz,c','Theta,Fz,c','Delta,Cz,c','Delta,Pz,c','Delta,Fz,c','Alpha,Cz,o','Alpha,Pz,o','Alpha,Fz,o','Beta,Cz,o','Beta,Pz,o','Beta,Fz,o','Theta,Cz,o','Theta,Pz,o','Theta,Fz,o','Delta,Cz,o','Delta,Pz,o','Delta,Fz,o','dif_Alpha,Cz,c-o','dif_Alpha,Pz,c-o','dif_Alpha,Fz,c-o','dif_Beta,Cz,c-o','dif_Beta,Pz,c-o','dif_Beta,Fz,c-o','dif_Theta,Cz,c-o','dif-Theta,Pz,c-o','dif-Theta,Fz,c-o','dif-Delta,Cz,c-o','dif-Delta,Pz,c-o','dif-Delta,Fz,c-o','Freq_Grav_Cz_c','Freq_Grav_Pz_c','Freq_Grav_Fz_c','Freq_Grav_Cz_o','Freq_Grav_Pz_o','Freq_Grav_Fz_o','dif-Freq_Grav_Cz_c-o','Age','dif-Freq_Grav_Fz_c-o','Time']\n",
    "feature_names = [feature_names[i] for i in II]\n",
    "# Perform cross-validation manually\n",
    "for train_index, test_index in group_kfold.split(data, re_targets, re_sbj_ids):\n",
    "    print('test_sbj')\n",
    "    print(sbj_ids[test_index])\n",
    "    X_train, X_test =  data[train_index,:], data[test_index,:]\n",
    "    y_train, y_test =  re_targets[train_index],  re_targets[test_index]\n",
    "    test_blocks = sbj_blocks[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    train_sbjs = re_sbj_ids[train_index]\n",
    "    test_blocks = sbj_blocks[test_index]\n",
    "     \n",
    "    rows_with_1_in_first = np.where(y_train == 0)[0]\n",
    "    rows_with_1_in_mid = np.where(y_train == 1)[0]\n",
    "    # Find rows where the second column is 1\n",
    "    rows_with_1_in_second = np.where(y_train == 1)[0]\n",
    "    rows_with_similarity = np.where(X_train[:,-1] ==  X_test[0,-1])[0]\n",
    "    \n",
    "    # Randomly select 5 indices from each group\n",
    "    random_rows_with_1_in_first_5 = np.random.choice(rows_with_1_in_first,2, replace=False)\n",
    "    random_rows_with_1_in_second_5 = np.random.choice(rows_with_1_in_second,2, replace=False)\n",
    "    X_val = np.concatenate(( X_train[ random_rows_with_1_in_first_5,:],X_train[random_rows_with_1_in_second_5,:]),axis=0)\n",
    "    y_val = np.concatenate(( y_train[ random_rows_with_1_in_first_5],y_train[random_rows_with_1_in_second_5]),axis=0)\n",
    "    X_train = np.delete(X_train, [  random_rows_with_1_in_first_5,random_rows_with_1_in_second_5 ], axis=0)\n",
    "    y_train = np.delete(y_train, [  random_rows_with_1_in_first_5,random_rows_with_1_in_second_5], axis=0)\n",
    "\n",
    "    if not (0 in y_train and 1 in y_train):\n",
    "        print(\"Skipping this fold because one of the classes is missing in y_train\")\n",
    "        continue  # Move to \n",
    "    train_pool = Pool(\n",
    "        data=X_train,       # Training data\n",
    "        label=y_train,      # Labels\n",
    "        feature_names=feature_names  # Assign feature names\n",
    "    )\n",
    "    val_pool = Pool(\n",
    "        data=X_val,\n",
    "        label=y_val,\n",
    "        feature_names=feature_names  # Optional: Consistent feature names for validation\n",
    "    )\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,  # Use Pool object for validation\n",
    "        plot=True  # Enable training visualization\n",
    "    )\n",
    "\n",
    "    model.plot_tree(\n",
    "        tree_idx=0,\n",
    "        pool=train_pool,  # Provide the Pool object with feature names\n",
    "    )\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    preds.append(y_pred)\n",
    "    y_tests.append(y_test)\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    n_match += np.sum(y_test==y_pred.T)\n",
    "    n_total += (y_pred).shape[0]\n",
    "    print('-------------------------')\n",
    "    print((y_test==y_pred.T).shape)\n",
    "    print(y_pred.shape)\n",
    "    print(y_test.shape)\n",
    "    print('--------------------------')\n",
    "    accuracy =  accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate the Root Mean Squared Error for this fold\n",
    "    acc.append(accuracy)\n",
    "    all_test_blocks.append(test_blocks)\n",
    "    \n",
    "    print(f\"Subject {i}, Fold Accuracy: {accuracy}\")\n",
    "    class_mapping = {0: \"High Per\", 1: \"Low per\"}\n",
    "      \n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_layout_engine(\"tight\")\n",
    "    shap.summary_plot(\n",
    "        shap_values[:, :,0],  # SHAP values for the class\n",
    "        features=X_train,  # The corresponding feature values\n",
    "        feature_names=feature_names,  # Names of features\n",
    "        class_names=[class_mapping[0], class_mapping[1]]  # Class labels'\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()  # This will automatically adjust the layout to avoid overlaps\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    if i==1:\n",
    "        feature_importances = np.zeros(model.get_feature_importance().shape)\n",
    "    feature_importances += model.get_feature_importance()\n",
    "    i +=1\n",
    "\n",
    "# Calculate the average MSE across all folds\n",
    "average_acc = np.mean(acc)\n",
    "print(f\"Average Accuracy across all folds: {average_acc}\")\n",
    "\n",
    "for feature_name, importance in zip(feature_names, feature_importances):\n",
    "    print(f\"{feature_name}: {importance}\")\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_train,       # Training data\n",
    "    label=y_train,      # Labels\n",
    "    feature_names=feature_names  # Assign feature names\n",
    ")\n",
    "val_pool = Pool(\n",
    "    data=X_val,\n",
    "    label=y_val,\n",
    "    feature_names=feature_names  # Optional: Consistent feature names for validation\n",
    ")\n",
    "\n",
    "# Train the XGBoost model on the training set\n",
    "#xgb_model.fit(X_train, y_train)\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,  # Use Pool object for validation\n",
    "    plot=True  # Enable training visualization\n",
    ")\n",
    "\n",
    "model.plot_tree(\n",
    "    tree_idx=1,\n",
    "    pool=train_pool,  # Provide the Pool object with feature names\n",
    ")\n",
    "\n",
    "y_tests_flat = np.concatenate(y_tests)\n",
    "preds_flat = np.concatenate([arr.flatten() for arr in preds])\n",
    "report = classification_report(y_tests_flat, preds_flat)\n",
    "falt_blocks = np.concatenate(all_test_blocks)\n",
    "# Print the classification report\n",
    "print(report)\n",
    "print(np.sum(preds_flat==y_tests_flat)/len(y_tests_flat))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
